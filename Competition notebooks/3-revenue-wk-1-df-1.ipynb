{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/1-revenue-all-df/df_1.pkl\n",
      "/kaggle/input/1-revenue-all-df/__results__.html\n",
      "/kaggle/input/1-revenue-all-df/df_5.pkl\n",
      "/kaggle/input/1-revenue-all-df/df_2.pkl\n",
      "/kaggle/input/1-revenue-all-df/df_6.pkl\n",
      "/kaggle/input/1-revenue-all-df/df_3.pkl\n",
      "/kaggle/input/1-revenue-all-df/__notebook__.ipynb\n",
      "/kaggle/input/1-revenue-all-df/df_7.pkl\n",
      "/kaggle/input/1-revenue-all-df/custom.css\n",
      "/kaggle/input/1-revenue-all-df/df_4.pkl\n",
      "/kaggle/input/1-revenue-all-df/__output__.json\n",
      "/kaggle/input/1-revenue-all-df/__results___files/__results___13_0.png\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc; gc.enable()\n",
    "\n",
    "import os, sys, gc, time, warnings, pickle, random, psutil\n",
    "from scipy import sparse\n",
    "from  datetime import datetime, timedelta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LinearRegression, Ridge,Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier,BaggingRegressor\n",
    "from sklearn.model_selection import cross_val_score, cross_validate,cross_val_predict\n",
    "from sklearn.model_selection import  train_test_split,KFold,StratifiedKFold,TimeSeriesSplit\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder,MinMaxScaler, RobustScaler\n",
    "from sklearn.preprocessing import KBinsDiscretizer, FunctionTransformer\n",
    "\n",
    "import category_encoders as ce\n",
    "\n",
    "from category_encoders import WOEEncoder, TargetEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from dask_ml.wrappers import Incremental\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "def get_memory_usage():\n",
    "    return np.round(psutil.Process(os.getpid()).memory_info()[0]/2.**30, 2)\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min >= np.iinfo(np.int8).min and c_max <= np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min >= np.iinfo(np.int16).min and c_max <= np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min >= np.iinfo(np.int32).min and c_max <= np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min >= np.iinfo(np.int64).min and c_max <= np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min >= np.finfo(np.float16).min and c_max <= np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min >= np.finfo(np.float32).min and c_max <= np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_20 = pd.read_pickle('/kaggle/input/part-1-new/df_20.pkl')\n",
    "# df_200 = pd.read_pickle('/kaggle/input/part-1-new/df_200.pkl')\n",
    "# df_ca_1 = pd.read_pickle('/kaggle/input/part-1-new/df_ca_1.pkl')\n",
    "df = pd.read_pickle('/kaggle/input/1-revenue-all-df/df_1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d</th>\n",
       "      <th>sales</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>wday</th>\n",
       "      <th>...</th>\n",
       "      <th>snap_CA</th>\n",
       "      <th>snap_TX</th>\n",
       "      <th>snap_WI</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>event_type_1</th>\n",
       "      <th>event_name_2</th>\n",
       "      <th>event_type_2</th>\n",
       "      <th>day</th>\n",
       "      <th>week</th>\n",
       "      <th>sell_price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-01-29</th>\n",
       "      <td>HOBBIES_1_141_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_141</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>11101</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>14.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-29</th>\n",
       "      <td>FOODS_3_587_WI_2_evaluation</td>\n",
       "      <td>FOODS_3_587</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>WI_2</td>\n",
       "      <td>WI</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11101</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>2.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-29</th>\n",
       "      <td>HOBBIES_1_345_WI_1_evaluation</td>\n",
       "      <td>HOBBIES_1_345</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>WI_1</td>\n",
       "      <td>WI</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11101</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>7.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-29</th>\n",
       "      <td>FOODS_3_263_TX_1_evaluation</td>\n",
       "      <td>FOODS_3_263</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>TX_1</td>\n",
       "      <td>TX</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>11101</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>2.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-29</th>\n",
       "      <td>HOBBIES_1_297_CA_3_evaluation</td>\n",
       "      <td>HOBBIES_1_297</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>11101</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>13.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-22</th>\n",
       "      <td>FOODS_3_377_TX_2_evaluation</td>\n",
       "      <td>FOODS_3_377</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>TX_2</td>\n",
       "      <td>TX</td>\n",
       "      <td>1941</td>\n",
       "      <td>18</td>\n",
       "      <td>11617</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>20</td>\n",
       "      <td>1.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-22</th>\n",
       "      <td>FOODS_3_389_TX_2_evaluation</td>\n",
       "      <td>FOODS_3_389</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>TX_2</td>\n",
       "      <td>TX</td>\n",
       "      <td>1941</td>\n",
       "      <td>27</td>\n",
       "      <td>11617</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>20</td>\n",
       "      <td>1.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-22</th>\n",
       "      <td>HOUSEHOLD_1_029_CA_3_evaluation</td>\n",
       "      <td>HOUSEHOLD_1_029</td>\n",
       "      <td>HOUSEHOLD_1</td>\n",
       "      <td>HOUSEHOLD</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1941</td>\n",
       "      <td>2</td>\n",
       "      <td>11617</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>20</td>\n",
       "      <td>7.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-22</th>\n",
       "      <td>FOODS_3_476_TX_2_evaluation</td>\n",
       "      <td>FOODS_3_476</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>TX_2</td>\n",
       "      <td>TX</td>\n",
       "      <td>1941</td>\n",
       "      <td>4</td>\n",
       "      <td>11617</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>20</td>\n",
       "      <td>5.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-22</th>\n",
       "      <td>FOODS_3_811_WI_3_evaluation</td>\n",
       "      <td>FOODS_3_811</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>WI_3</td>\n",
       "      <td>WI</td>\n",
       "      <td>1941</td>\n",
       "      <td>31</td>\n",
       "      <td>11617</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>20</td>\n",
       "      <td>1.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>852648 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         id          item_id      dept_id  \\\n",
       "date                                                                        \n",
       "2011-01-29    HOBBIES_1_141_CA_1_evaluation    HOBBIES_1_141    HOBBIES_1   \n",
       "2011-01-29      FOODS_3_587_WI_2_evaluation      FOODS_3_587      FOODS_3   \n",
       "2011-01-29    HOBBIES_1_345_WI_1_evaluation    HOBBIES_1_345    HOBBIES_1   \n",
       "2011-01-29      FOODS_3_263_TX_1_evaluation      FOODS_3_263      FOODS_3   \n",
       "2011-01-29    HOBBIES_1_297_CA_3_evaluation    HOBBIES_1_297    HOBBIES_1   \n",
       "...                                     ...              ...          ...   \n",
       "2016-05-22      FOODS_3_377_TX_2_evaluation      FOODS_3_377      FOODS_3   \n",
       "2016-05-22      FOODS_3_389_TX_2_evaluation      FOODS_3_389      FOODS_3   \n",
       "2016-05-22  HOUSEHOLD_1_029_CA_3_evaluation  HOUSEHOLD_1_029  HOUSEHOLD_1   \n",
       "2016-05-22      FOODS_3_476_TX_2_evaluation      FOODS_3_476      FOODS_3   \n",
       "2016-05-22      FOODS_3_811_WI_3_evaluation      FOODS_3_811      FOODS_3   \n",
       "\n",
       "               cat_id store_id state_id     d  sales  wm_yr_wk  wday  ...  \\\n",
       "date                                                                  ...   \n",
       "2011-01-29    HOBBIES     CA_1       CA     1      2     11101     1  ...   \n",
       "2011-01-29      FOODS     WI_2       WI     1      0     11101     1  ...   \n",
       "2011-01-29    HOBBIES     WI_1       WI     1      0     11101     1  ...   \n",
       "2011-01-29      FOODS     TX_1       TX     1      4     11101     1  ...   \n",
       "2011-01-29    HOBBIES     CA_3       CA     1      2     11101     1  ...   \n",
       "...               ...      ...      ...   ...    ...       ...   ...  ...   \n",
       "2016-05-22      FOODS     TX_2       TX  1941     18     11617     2  ...   \n",
       "2016-05-22      FOODS     TX_2       TX  1941     27     11617     2  ...   \n",
       "2016-05-22  HOUSEHOLD     CA_3       CA  1941      2     11617     2  ...   \n",
       "2016-05-22      FOODS     TX_2       TX  1941      4     11617     2  ...   \n",
       "2016-05-22      FOODS     WI_3       WI  1941     31     11617     2  ...   \n",
       "\n",
       "            snap_CA  snap_TX  snap_WI  event_name_1  event_type_1  \\\n",
       "date                                                                \n",
       "2011-01-29        0        0        0             0             0   \n",
       "2011-01-29        0        0        0             0             0   \n",
       "2011-01-29        0        0        0             0             0   \n",
       "2011-01-29        0        0        0             0             0   \n",
       "2011-01-29        0        0        0             0             0   \n",
       "...             ...      ...      ...           ...           ...   \n",
       "2016-05-22        0        0        0             0             0   \n",
       "2016-05-22        0        0        0             0             0   \n",
       "2016-05-22        0        0        0             0             0   \n",
       "2016-05-22        0        0        0             0             0   \n",
       "2016-05-22        0        0        0             0             0   \n",
       "\n",
       "            event_name_2  event_type_2  day  week  sell_price  \n",
       "date                                                           \n",
       "2011-01-29             0             0   29     4       14.26  \n",
       "2011-01-29             0             0   29     4        2.48  \n",
       "2011-01-29             0             0   29     4        7.44  \n",
       "2011-01-29             0             0   29     4        2.68  \n",
       "2011-01-29             0             0   29     4       13.00  \n",
       "...                  ...           ...  ...   ...         ...  \n",
       "2016-05-22             0             0   22    20        1.68  \n",
       "2016-05-22             0             0   22    20        1.68  \n",
       "2016-05-22             0             0   22    20        7.48  \n",
       "2016-05-22             0             0   22    20        5.98  \n",
       "2016-05-22             0             0   22    20        1.88  \n",
       "\n",
       "[852648 rows x 22 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 852648 entries, 2011-01-29 to 2016-05-22\n",
      "Data columns (total 22 columns):\n",
      " #   Column        Non-Null Count   Dtype   \n",
      "---  ------        --------------   -----   \n",
      " 0   id            852648 non-null  category\n",
      " 1   item_id       852648 non-null  category\n",
      " 2   dept_id       852648 non-null  category\n",
      " 3   cat_id        852648 non-null  category\n",
      " 4   store_id      852648 non-null  category\n",
      " 5   state_id      852648 non-null  category\n",
      " 6   d             852648 non-null  int16   \n",
      " 7   sales         852648 non-null  int16   \n",
      " 8   wm_yr_wk      852648 non-null  int16   \n",
      " 9   wday          852648 non-null  int8    \n",
      " 10  month         852648 non-null  int8    \n",
      " 11  year          852648 non-null  int16   \n",
      " 12  snap_CA       852648 non-null  int8    \n",
      " 13  snap_TX       852648 non-null  int8    \n",
      " 14  snap_WI       852648 non-null  int8    \n",
      " 15  event_name_1  852648 non-null  int8    \n",
      " 16  event_type_1  852648 non-null  int8    \n",
      " 17  event_name_2  852648 non-null  int8    \n",
      " 18  event_type_2  852648 non-null  int8    \n",
      " 19  day           852648 non-null  int8    \n",
      " 20  week          852648 non-null  int8    \n",
      " 21  sell_price    852648 non-null  float32 \n",
      "dtypes: category(6), float32(1), int16(4), int8(11)\n",
      "memory usage: 33.3 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## 2. Feature Engineering\n",
    "### 2.1. Lag Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_features(df):\n",
    "    '''\n",
    "    Function returns data frame with lag features.\n",
    "    \n",
    "    '''\n",
    "    #create data frame from data frame for lag features\n",
    "    lag_df = df[['id','item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'd', 'sales']]\n",
    "    lag_group = lag_df.groupby(['id','item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])\n",
    "    \n",
    "    # cannot use .pct_change() because of some 0 values, which give a lot of 'inf' and 'NaN'\n",
    "    # create lag(shift) features: this represents % cahnge of sales 'shift' days ago\n",
    "    shifts = [7,8,9,10,11,12,13,14,28,35,42]\n",
    "    lag_diff = lag_group['sales'].transform(lambda x: x.diff().astype(np.float32))\n",
    "    lag_max = lag_group['sales'].transform(lambda x: x.mean().astype(np.float32))\n",
    "    lag_df['pct_norm'] = (lag_diff/lag_max).astype(np.float32)\n",
    "    \n",
    "    for i in shifts:\n",
    "        lag_df[f'pct_shift_{i}'] = lag_group['pct_norm'].shift(i)\n",
    "    \n",
    "    \n",
    "    # use shift min=7 with windows for pct change of rolling mean and std dev. calc  \n",
    "    windows = [7,14,28]\n",
    "    shifts = [7, 14] \n",
    "    for i in shifts:\n",
    "        \n",
    "        for j in windows:\n",
    "            lag_df[f'pct_rmean{j}_lag{i}'] = \\\n",
    "            lag_group[f'pct_shift_{i}'].transform(lambda x: x.rolling(j).mean().astype(np.float32))\n",
    "            \n",
    "            lag_df[f'pct_rstd{j}_lag{i}'] = \\\n",
    "            lag_group[f'pct_shift_{i}'].transform(lambda x: x.rolling(j).std().astype(np.float32))\n",
    "\n",
    "            \n",
    "    del lag_df['pct_norm']\n",
    "    gc.collect()\n",
    "    \n",
    "    return lag_df\n",
    "\n",
    "\n",
    "def merge_lag_df(df,lag_df):\n",
    "    \n",
    "    '''\n",
    "    Function returns merged df with lag features df.\n",
    "    '''\n",
    "    ### merge lag features with data frame\n",
    "    \n",
    "    lag_df.drop(columns = 'sales',axis=1,inplace=True)\n",
    "    df = df.merge(lag_df, \n",
    "                  on=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'd'], \n",
    "                  how='left', left_index=True)\n",
    "    gc.collect()\n",
    "\n",
    "    return df\n",
    "\n",
    "def remove_nan(df):\n",
    "    '''\n",
    "    Function drops NaN, created by lag features.\n",
    "    '''\n",
    "    #df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    df = df.dropna()\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "SHIFT = 7\n",
    "\n",
    "### 2.2. Price Features\n",
    "def price_features(df):\n",
    "    \n",
    "    '''\n",
    "    Function returns data frame with price features(moving average = 14 days).\n",
    "    '''\n",
    "#     price_group = df['sell_price'].pct_change().rolling(14)\n",
    "#     df['pct_price_mean']= price_group.mean().shift(SHIFT)\n",
    "#     df['pct_price_max']= price_group.max().shift(SHIFT)\n",
    "#     df['pct_price_min']= price_group.min().shift(SHIFT)\n",
    "#     df['pct_price_std']= price_group.std().shift(SHIFT)\n",
    "    \n",
    "    price_group = df.groupby(['id','item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])\n",
    "\n",
    "    df['pct_price'] = price_group['sell_price'].pct_change().astype(np.float32)\n",
    "    \n",
    "    df['pct_price_mean'] = price_group['pct_price'].transform(lambda x: x.rolling(14).mean().astype(np.float32))\n",
    "    df['pct_price_mean'] = price_group['pct_price_mean'].shift(SHIFT)\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    df['pct_price_std'] = price_group['pct_price'].transform(lambda x: x.rolling(14).std().astype(np.float32))\n",
    "    df['pct_price_std'] = price_group['pct_price_std'].shift(SHIFT)\n",
    "    \n",
    "    df['pct_price_max'] = price_group['pct_price'].transform(lambda x: x.rolling(14).max().astype(np.float32))\n",
    "    df['pct_price_max'] = price_group['pct_price_max'].shift(SHIFT)\n",
    "    \n",
    "    df['pct_price_min'] = price_group['pct_price'].transform(lambda x: x.rolling(14).min().astype(np.float32))\n",
    "    df['pct_price_min'] = price_group['pct_price_min'].shift(SHIFT)\n",
    "    \n",
    "    del df['pct_price']\n",
    "    gc.collect()\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## 3. Baseline Model\n",
    "## 3.1. LIGHT GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockingTimeSeriesSplit():\n",
    "    def __init__(self, n_splits):\n",
    "        self.n_splits = n_splits\n",
    "    \n",
    "    def get_n_splits(self, X, y, groups):\n",
    "        return self.n_splits\n",
    "    \n",
    "    def split(self, X, y=None, groups=None):\n",
    "        n_samples = len(X)\n",
    "        k_fold_size = n_samples // self.n_splits\n",
    "        indices = np.arange(n_samples)\n",
    "\n",
    "        margin = 0\n",
    "        for i in range(self.n_splits):\n",
    "            start = i * k_fold_size\n",
    "            stop = start + k_fold_size\n",
    "            mid = int(0.8 * (stop - start)) + start\n",
    "            yield indices[start: mid], indices[mid + margin: stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lgb(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    '''\n",
    "    Function prints RMSE for train and test data.\n",
    "    '''\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create X, Y\n",
    "    # 'wday' duplicates 'weekday'- put in remove_col\n",
    "#     remove_cols = ['sales', 'revenue'] \n",
    "    \n",
    "#     cat_cols = ['id','item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
    "#     num_cols = [c for c in df.columns.tolist() if c not in (remove_cols+cat_cols)]\n",
    "#     used_col = cat_cols+num_cols\n",
    "    \n",
    "    \n",
    "#     y = df['sales']\n",
    "#     X = df[used_col]\n",
    "#     del df\n",
    "#     gc.collect()\n",
    "    \n",
    "    \n",
    "#     # Split train, test(last 7 days) # new\n",
    "#     y_train, y_test = y[:'2016-05-15'], y['2016-05-16':]\n",
    "#     del y\n",
    "#     gc.collect()\n",
    "\n",
    "#     X_train, X_test = X[:'2016-05-15'], X['2016-05-16':]\n",
    "#     del X\n",
    "#     gc.collect()\n",
    "    \n",
    "    ### Make numerical pipe\n",
    "    imputer = SimpleImputer()\n",
    "    scaler = RobustScaler()\n",
    "    pca = PCA(n_components=0.9, random_state = 42)\n",
    "    num_pipe = make_pipeline(imputer,scaler,pca)\n",
    "    \n",
    "    ### Make categorical pipe\n",
    "    #encoder = ce.TargetEncoder(cols=cat_cols, handle_missing=\"value\")\n",
    "    encoder = ce.cat_boost.CatBoostEncoder(cols=cat_cols, handle_missing=\"value\")\n",
    "    cat_pipe = make_pipeline(encoder)\n",
    "    \n",
    "    \n",
    "    # Define preprocessor\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[('num', num_pipe, num_cols),\n",
    "                      ('cat', cat_pipe, cat_cols)])\n",
    "   \n",
    "    # Define model\n",
    "    model = lgb.LGBMRegressor(objective=\"poisson\", \n",
    "                              metric=\"rmse\",\n",
    "                              force_row_wise = True,\n",
    "                              learning_rate = 0.02, #0.04,\n",
    "                              sub_row = 0.75,\n",
    "                              bagging_freq = 1,\n",
    "                              lambda_l2 = 0.1,\n",
    "                              verbosity = 1,\n",
    "                              num_iterations = 100,\n",
    "                              num_leaves = 20,#40,\n",
    "                              max_depth = 5, #20,\n",
    "                              min_data_in_leaf = 10)#11)\n",
    "    \n",
    "    # Make main pipeline\n",
    "    pipe = make_pipeline(preprocessor, model)\n",
    "\n",
    "    ### cross_validate\n",
    "    \n",
    "#     block = BlockingTimeSeriesSplit(n_splits=5)\n",
    "    #ts_cv = TimeSeriesSplit(n_splits=10)\n",
    "    #kfcv = KFold(n_splits=5)\n",
    "\n",
    "#     scores = cross_validate(pipe, X_train, y_train, cv=block, \n",
    "#                              scoring='neg_root_mean_squared_error',\n",
    "#                              n_jobs=-1,return_train_score=True)\n",
    "    #sorted(scores.keys()) ['fit_time', 'score_time', 'test_score', 'train_score']\n",
    "\n",
    "#     print('Train scores:', scores['train_score'])\n",
    "#     print('Test scores:', scores['test_score'])\n",
    "\n",
    "#     print('Train mean:',scores['train_score'].mean(),\"+/-\",scores['train_score'].std())\n",
    "#     print('Test mean:',scores['test_score'].mean(),\"+/-\",scores['test_score'].std())\n",
    "    \n",
    "#     del scores\n",
    "#     gc.collect()\n",
    "    \n",
    "    #### fit and predict\n",
    "\n",
    "    pipe.fit(X_train,y_train)\n",
    "    y_pred_train=pipe.predict(X_train)\n",
    "    y_pred=pipe.predict(X_test)\n",
    "\n",
    "    print('--------------------')\n",
    "    print(\"RMSE_train:\",(mean_squared_error(y_train.values, y_pred_train))**0.5)\n",
    "    print(\"RMSE_test:\",(mean_squared_error(y_test.values, y_pred))**0.5)\n",
    "    print('--------------------')\n",
    "    print()\n",
    "    print('Naive benchmarks:')\n",
    "    print('Predicted Test week average sales:',y_pred.mean())\n",
    "    print('True Average Sales at:')\n",
    "    print('Test week:',y_test.values.mean())\n",
    "#     print('Week before test:', y['2016-04-10':'2016-04-16'].values.mean())\n",
    "#     print('Last year at test week:', y['2015-04-20':'2015-04-26'].values.mean())\n",
    "    print('--------------------')\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print('time:', (end_time-start_time)/60, 'min')\n",
    "\n",
    "    \n",
    "#     print('Sales')\n",
    "\n",
    "#     print('Predicted Test week:',np.round(y_pred.tolist()))\n",
    "#     print('True Test week:',y_test.values.tolist())\n",
    "#     print('Week before test:', y['2016-04-11':'2016-04-17'].values.tolist())\n",
    "#     print('Last year @ test week:', y['2015-04-20':'2015-04-26'].values.tolist())\n",
    "    \n",
    "    \n",
    "    del y_pred_train\n",
    "    gc.collect()\n",
    "\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## 4. Vanilla Linear Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v_lin_reg(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    '''\n",
    "    Vanilla linear reg model\n",
    "    Function prints RMSE for train and test data.\n",
    "    '''\n",
    "    start_time = time.time()\n",
    "    \n",
    "    ### Make numerical pipe\n",
    "    imputer = SimpleImputer()\n",
    "    scaler = RobustScaler()\n",
    "    pca = PCA(n_components=0.9, random_state = 42)\n",
    "    num_pipe = make_pipeline(imputer,scaler,pca)\n",
    "    \n",
    "    ### Make categorical pipe\n",
    "    #encoder = ce.TargetEncoder(cols=cat_cols, handle_missing=\"value\")\n",
    "    encoder = ce.cat_boost.CatBoostEncoder(cols=cat_cols, handle_missing=\"value\")\n",
    "    cat_pipe = make_pipeline(encoder)\n",
    "    \n",
    "    \n",
    "    # Define preprocessor\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[('num', num_pipe, num_cols),\n",
    "                      ('cat', cat_pipe, cat_cols)])\n",
    "   \n",
    "    # Define model\n",
    "    model = LinearRegression(n_jobs=-1)\n",
    "    \n",
    "    # Make main pipeline\n",
    "    pipe = make_pipeline(preprocessor, model)\n",
    "\n",
    "    ### cross_validate\n",
    "    \n",
    "    #block = BlockingTimeSeriesSplit(n_splits=5)\n",
    "    ##ts_cv = TimeSeriesSplit(n_splits=10)\n",
    "    ##kfcv = KFold(n_splits=5)\n",
    "\n",
    "#     scores = cross_validate(pipe, X_train, y_train, cv=block, \n",
    "#                              scoring='neg_root_mean_squared_error',\n",
    "#                              return_train_score=True) #n_jobs=-1,\n",
    "    ###sorted(scores.keys()) ['fit_time', 'score_time', 'test_score', 'train_score']\n",
    "\n",
    "#     print('Train scores:', scores['train_score'])\n",
    "#     print('Test scores:', scores['test_score'])\n",
    "\n",
    "#     print('Train mean:',scores['train_score'].mean(),\"+/-\",scores['train_score'].std())\n",
    "#     print('Test mean:',scores['test_score'].mean(),\"+/-\",scores['test_score'].std())\n",
    "    \n",
    "#     del scores\n",
    "#     gc.collect()\n",
    "    \n",
    "    #### fit and predict\n",
    "\n",
    "    pipe.fit(X_train,y_train)\n",
    "    y_pred_train=pipe.predict(X_train)\n",
    "    y_pred=pipe.predict(X_test)\n",
    "\n",
    "    print('--------------------')\n",
    "    print(\"RMSE_train:\",(mean_squared_error(y_train.values, y_pred_train))**0.5)\n",
    "    print(\"RMSE_test:\",(mean_squared_error(y_test.values, y_pred))**0.5)\n",
    "    print('--------------------')\n",
    "    print()\n",
    "    print('Naive benchmarks:')\n",
    "    print('Predicted Test week average sales:',y_pred.mean())\n",
    "    print('True Average Sales at:')\n",
    "    print('Test week:',y_test.values.mean())\n",
    "#     print('Week before test:', y['2016-04-10':'2016-04-16'].values.mean())\n",
    "#     print('Last year at test week:', y['2015-04-20':'2015-04-26'].values.mean())\n",
    "    print('--------------------')\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print('time:', (end_time-start_time)/60, 'min')\n",
    "    \n",
    "    del y_pred_train\n",
    "    gc.collect()\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# est = SGDClassifier(loss='log', penalty='l2', tol=1e-3)\n",
    "# inc = Incremental(est, scoring='accuracy')\n",
    "# inc.fit(X_train, y_train, classes=classes)\n",
    "# inc.score(X_test, y_test)\n",
    "# inc.predict(X_test)  # Predict produces lazy dask arrays\n",
    "# inc.predict(X_test)[:100].compute()  # call compute to get results\n",
    "# inc.score(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "def sgd(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    '''\n",
    "    Function prints RMSE for train and test data.\n",
    "    '''\n",
    "    start_time = time.time()\n",
    "    \n",
    "    \n",
    "    ### Make numerical pipe\n",
    "    imputer = SimpleImputer()\n",
    "    scaler = RobustScaler()\n",
    "    pca = PCA(n_components=0.9, random_state = 42)\n",
    "    num_pipe = make_pipeline(imputer,scaler,pca)\n",
    "    \n",
    "    ### Make categorical pipe\n",
    "    #encoder = ce.TargetEncoder(cols=cat_cols, handle_missing=\"value\")\n",
    "    encoder = ce.cat_boost.CatBoostEncoder(cols=cat_cols, handle_missing=\"value\")\n",
    "    cat_pipe = make_pipeline(encoder)\n",
    "    \n",
    "    \n",
    "    # Define preprocessor\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[('num', num_pipe, num_cols),\n",
    "                      ('cat', cat_pipe, cat_cols)])\n",
    "   \n",
    "    # Define model\n",
    "#     model = LinearRegression()\n",
    "    model = SGDRegressor(shuffle=False, \n",
    "                         penalty='elasticnet',\n",
    "                         loss='epsilon_insensitive',\n",
    "                         learning_rate='adaptive')\n",
    "    \n",
    "    #inc = Incremental(model) #, scoring='accuracy')\n",
    "\n",
    "\n",
    "    # Make main pipeline\n",
    "    pipe = make_pipeline(preprocessor, model)\n",
    "\n",
    "    ### cross_validate\n",
    "    \n",
    "    block = BlockingTimeSeriesSplit(n_splits=5)\n",
    "    #ts_cv = TimeSeriesSplit(n_splits=10)\n",
    "    #kfcv = KFold(n_splits=5)\n",
    "    \n",
    "    \n",
    "    \n",
    "    scores = cross_validate(pipe, X_train, y_train, cv=block, \n",
    "                             scoring='neg_root_mean_squared_error',\n",
    "                             return_train_score=True) #n_jobs=-1,\n",
    "    #sorted(scores.keys()) ['fit_time', 'score_time', 'test_score', 'train_score']\n",
    "\n",
    "    print('Train scores:', scores['train_score'])\n",
    "    print('Test scores:', scores['test_score'])\n",
    "    \n",
    "    print('Train mean:',scores['train_score'].mean(),\"+/-\",scores['train_score'].std())\n",
    "    print('Test mean:',scores['test_score'].mean(),\"+/-\",scores['test_score'].std())\n",
    "    \n",
    "    del scores\n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "    #### fit and predict\n",
    "    pipe.fit(X_train,y_train)\n",
    "    \n",
    "    y_pred_train=pipe.predict(X_train)\n",
    "    y_pred=pipe.predict(X_test)\n",
    "\n",
    "    print('--------------------')\n",
    "    print(\"RMSE_train:\",(mean_squared_error(y_train.values, y_pred_train))**0.5)\n",
    "    print(\"RMSE_test:\",(mean_squared_error(y_test.values, y_pred))**0.5)\n",
    "    print('--------------------')\n",
    "    print()\n",
    "    print('Naive benchmarks:')\n",
    "    print('Predicted Test week average sales:',y_pred.mean())\n",
    "    print('True Average Sales at:')\n",
    "    print('Test week:',y_test.values.mean())\n",
    "#     print('Week before test:', y['2016-04-10':'2016-04-16'].values.mean())\n",
    "#     print('Last year at test week:', y['2015-04-20':'2015-04-26'].values.mean())\n",
    "    print('--------------------')\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print('time:', (end_time-start_time)/60, 'min')\n",
    "    \n",
    "    del y_pred_train\n",
    "    gc.collect()\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "1. ### test vanilla linear reg on df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### train: days [1; 1913] or [2011-01-29; 2016-04-24]\n",
    "\n",
    "### test week_1: days [1914 : 1920] or [2016-04-25 : 2016-05-01]\n",
    "df = df.loc[(df.d<=1913)| ((df.d>=1914)&(df.d<=1920))]\n",
    "#df.loc[(df.d<=1913)| ((df.d>=1914)&(df.d<=1920))][:'2016-04-24'] # train split\n",
    "#df.loc[(df.d<=1913)| ((df.d>=1914)&(df.d<=1920))]['2016-04-25':] # test split\n",
    "\n",
    "\n",
    "### test week_2: days [1921 : 1927] or [2016-05-02 : 2016-05-08]\n",
    "# df = df.loc[(df.d<=1913)| ((df.d>=1921)&(df.d<=1927))]\n",
    "# #df.loc[(df.d<=1913)| ((df.d>=1921)&(df.d<=1927))][:'2016-04-24'] # train split\n",
    "# #df.loc[(df.d<=1913)| ((df.d>=1921)&(df.d<=1927))]['2016-04-25':] # test split\n",
    "\n",
    "\n",
    "### test week_3: days [1928 : 1934] or [2016-05-09 : 2016-05-15]\n",
    "# df = df.loc[(df.d<=1913)| ((df.d>=1928)&(df.d<=1934))]\n",
    "# #df.loc[(df.d<=1913)| ((df.d>=1928)&(df.d<=1934))][:'2016-04-24'] # train split\n",
    "# #df.loc[(df.d<=1913)| ((df.d>=1928)&(df.d<=1934))]['2016-04-25':] # test split\n",
    "\n",
    "### test week_4: days [1935 : 1941] or [2016-05-16 : 2016-05-22]\n",
    "# df = df.loc[(df.d<=1913)| ((df.d>=1935)&(df.d<=1941))]\n",
    "# #df.loc[(df.d<=1913)| ((df.d>=1935)&(df.d<=1941))][:'2016-04-24'] # train split\n",
    "# #df.loc[(df.d<=1913)| ((df.d>=1935)&(df.d<=1941))]['2016-04-25':] # test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0.27414055665334064 min\n"
     ]
    }
   ],
   "source": [
    "### create features: \n",
    "start_time = time.time()\n",
    "\n",
    "lag = lag_features(df)\n",
    "df = merge_lag_df(df, lag)\n",
    "del lag\n",
    "df = price_features(df)\n",
    "df = remove_nan(df)\n",
    "\n",
    "end_time = time.time()\n",
    "print('time:', (end_time-start_time)/60, 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 819304 entries, 2011-03-13 to 2016-05-01\n",
      "Data columns (total 49 columns):\n",
      " #   Column             Non-Null Count   Dtype   \n",
      "---  ------             --------------   -----   \n",
      " 0   id                 819304 non-null  category\n",
      " 1   item_id            819304 non-null  category\n",
      " 2   dept_id            819304 non-null  category\n",
      " 3   cat_id             819304 non-null  category\n",
      " 4   store_id           819304 non-null  category\n",
      " 5   state_id           819304 non-null  category\n",
      " 6   d                  819304 non-null  int16   \n",
      " 7   sales              819304 non-null  int16   \n",
      " 8   wm_yr_wk           819304 non-null  int16   \n",
      " 9   wday               819304 non-null  int8    \n",
      " 10  month              819304 non-null  int8    \n",
      " 11  year               819304 non-null  int16   \n",
      " 12  snap_CA            819304 non-null  int8    \n",
      " 13  snap_TX            819304 non-null  int8    \n",
      " 14  snap_WI            819304 non-null  int8    \n",
      " 15  event_name_1       819304 non-null  int8    \n",
      " 16  event_type_1       819304 non-null  int8    \n",
      " 17  event_name_2       819304 non-null  int8    \n",
      " 18  event_type_2       819304 non-null  int8    \n",
      " 19  day                819304 non-null  int8    \n",
      " 20  week               819304 non-null  int8    \n",
      " 21  sell_price         819304 non-null  float32 \n",
      " 22  pct_shift_7        819304 non-null  float32 \n",
      " 23  pct_shift_8        819304 non-null  float32 \n",
      " 24  pct_shift_9        819304 non-null  float32 \n",
      " 25  pct_shift_10       819304 non-null  float32 \n",
      " 26  pct_shift_11       819304 non-null  float32 \n",
      " 27  pct_shift_12       819304 non-null  float32 \n",
      " 28  pct_shift_13       819304 non-null  float32 \n",
      " 29  pct_shift_14       819304 non-null  float32 \n",
      " 30  pct_shift_28       819304 non-null  float32 \n",
      " 31  pct_shift_35       819304 non-null  float32 \n",
      " 32  pct_shift_42       819304 non-null  float32 \n",
      " 33  pct_rmean7_lag7    819304 non-null  float32 \n",
      " 34  pct_rstd7_lag7     819304 non-null  float32 \n",
      " 35  pct_rmean14_lag7   819304 non-null  float32 \n",
      " 36  pct_rstd14_lag7    819304 non-null  float32 \n",
      " 37  pct_rmean28_lag7   819304 non-null  float32 \n",
      " 38  pct_rstd28_lag7    819304 non-null  float32 \n",
      " 39  pct_rmean7_lag14   819304 non-null  float32 \n",
      " 40  pct_rstd7_lag14    819304 non-null  float32 \n",
      " 41  pct_rmean14_lag14  819304 non-null  float32 \n",
      " 42  pct_rstd14_lag14   819304 non-null  float32 \n",
      " 43  pct_rmean28_lag14  819304 non-null  float32 \n",
      " 44  pct_rstd28_lag14   819304 non-null  float32 \n",
      " 45  pct_price_mean     819304 non-null  float32 \n",
      " 46  pct_price_std      819304 non-null  float32 \n",
      " 47  pct_price_max      819304 non-null  float32 \n",
      " 48  pct_price_min      819304 non-null  float32 \n",
      "dtypes: category(6), float32(28), int16(4), int8(11)\n",
      "memory usage: 116.4 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 72.69 Mb (37.6% reduction)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d</th>\n",
       "      <th>sales</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>wday</th>\n",
       "      <th>...</th>\n",
       "      <th>pct_rmean7_lag14</th>\n",
       "      <th>pct_rstd7_lag14</th>\n",
       "      <th>pct_rmean14_lag14</th>\n",
       "      <th>pct_rstd14_lag14</th>\n",
       "      <th>pct_rmean28_lag14</th>\n",
       "      <th>pct_rstd28_lag14</th>\n",
       "      <th>pct_price_mean</th>\n",
       "      <th>pct_price_std</th>\n",
       "      <th>pct_price_max</th>\n",
       "      <th>pct_price_min</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-03-13</th>\n",
       "      <td>FOODS_2_285_WI_2_evaluation</td>\n",
       "      <td>FOODS_2_285</td>\n",
       "      <td>FOODS_2</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>WI_2</td>\n",
       "      <td>WI</td>\n",
       "      <td>44</td>\n",
       "      <td>4</td>\n",
       "      <td>11107</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.479248</td>\n",
       "      <td>0.040680</td>\n",
       "      <td>0.491211</td>\n",
       "      <td>0.015251</td>\n",
       "      <td>0.382324</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-03-13</th>\n",
       "      <td>FOODS_2_368_WI_2_evaluation</td>\n",
       "      <td>FOODS_2_368</td>\n",
       "      <td>FOODS_2</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>WI_2</td>\n",
       "      <td>WI</td>\n",
       "      <td>44</td>\n",
       "      <td>11</td>\n",
       "      <td>11107</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>1.176758</td>\n",
       "      <td>-0.294922</td>\n",
       "      <td>2.830078</td>\n",
       "      <td>0.016388</td>\n",
       "      <td>3.205078</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-03-13</th>\n",
       "      <td>FOODS_3_572_CA_3_evaluation</td>\n",
       "      <td>FOODS_3_572</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>44</td>\n",
       "      <td>10</td>\n",
       "      <td>11107</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081848</td>\n",
       "      <td>0.466553</td>\n",
       "      <td>-0.040924</td>\n",
       "      <td>0.540039</td>\n",
       "      <td>-0.025574</td>\n",
       "      <td>0.492920</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-03-13</th>\n",
       "      <td>FOODS_3_389_CA_3_evaluation</td>\n",
       "      <td>FOODS_3_389</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>44</td>\n",
       "      <td>19</td>\n",
       "      <td>11107</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039124</td>\n",
       "      <td>0.437744</td>\n",
       "      <td>-0.007820</td>\n",
       "      <td>0.530762</td>\n",
       "      <td>0.017609</td>\n",
       "      <td>0.471924</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-03-13</th>\n",
       "      <td>FOODS_3_458_CA_3_evaluation</td>\n",
       "      <td>FOODS_3_458</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>44</td>\n",
       "      <td>29</td>\n",
       "      <td>11107</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044952</td>\n",
       "      <td>0.427979</td>\n",
       "      <td>0.028091</td>\n",
       "      <td>0.378662</td>\n",
       "      <td>-0.002810</td>\n",
       "      <td>0.410889</td>\n",
       "      <td>0.001553</td>\n",
       "      <td>0.00581</td>\n",
       "      <td>0.021744</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-01</th>\n",
       "      <td>HOBBIES_1_155_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_155</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>1920</td>\n",
       "      <td>2</td>\n",
       "      <td>11614</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.101807</td>\n",
       "      <td>1.247070</td>\n",
       "      <td>0.025452</td>\n",
       "      <td>1.122070</td>\n",
       "      <td>-0.012726</td>\n",
       "      <td>1.038086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-01</th>\n",
       "      <td>FOODS_3_586_WI_2_evaluation</td>\n",
       "      <td>FOODS_3_586</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>WI_2</td>\n",
       "      <td>WI</td>\n",
       "      <td>1920</td>\n",
       "      <td>13</td>\n",
       "      <td>11614</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038239</td>\n",
       "      <td>0.389893</td>\n",
       "      <td>0.019119</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>0.016724</td>\n",
       "      <td>0.449219</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-01</th>\n",
       "      <td>HOBBIES_1_147_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_147</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>1920</td>\n",
       "      <td>8</td>\n",
       "      <td>11614</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.152466</td>\n",
       "      <td>0.814453</td>\n",
       "      <td>-0.060974</td>\n",
       "      <td>0.787109</td>\n",
       "      <td>0.007622</td>\n",
       "      <td>1.285156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-01</th>\n",
       "      <td>HOUSEHOLD_1_096_CA_3_evaluation</td>\n",
       "      <td>HOUSEHOLD_1_096</td>\n",
       "      <td>HOUSEHOLD_1</td>\n",
       "      <td>HOUSEHOLD</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1920</td>\n",
       "      <td>23</td>\n",
       "      <td>11614</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040649</td>\n",
       "      <td>0.775879</td>\n",
       "      <td>0.045746</td>\n",
       "      <td>0.669434</td>\n",
       "      <td>0.012703</td>\n",
       "      <td>0.601074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-01</th>\n",
       "      <td>FOODS_3_154_WI_2_evaluation</td>\n",
       "      <td>FOODS_3_154</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>WI_2</td>\n",
       "      <td>WI</td>\n",
       "      <td>1920</td>\n",
       "      <td>15</td>\n",
       "      <td>11614</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048920</td>\n",
       "      <td>0.984863</td>\n",
       "      <td>0.030579</td>\n",
       "      <td>0.902832</td>\n",
       "      <td>0.052002</td>\n",
       "      <td>0.949219</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>819304 rows Ã— 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         id          item_id      dept_id  \\\n",
       "date                                                                        \n",
       "2011-03-13      FOODS_2_285_WI_2_evaluation      FOODS_2_285      FOODS_2   \n",
       "2011-03-13      FOODS_2_368_WI_2_evaluation      FOODS_2_368      FOODS_2   \n",
       "2011-03-13      FOODS_3_572_CA_3_evaluation      FOODS_3_572      FOODS_3   \n",
       "2011-03-13      FOODS_3_389_CA_3_evaluation      FOODS_3_389      FOODS_3   \n",
       "2011-03-13      FOODS_3_458_CA_3_evaluation      FOODS_3_458      FOODS_3   \n",
       "...                                     ...              ...          ...   \n",
       "2016-05-01    HOBBIES_1_155_CA_1_evaluation    HOBBIES_1_155    HOBBIES_1   \n",
       "2016-05-01      FOODS_3_586_WI_2_evaluation      FOODS_3_586      FOODS_3   \n",
       "2016-05-01    HOBBIES_1_147_CA_1_evaluation    HOBBIES_1_147    HOBBIES_1   \n",
       "2016-05-01  HOUSEHOLD_1_096_CA_3_evaluation  HOUSEHOLD_1_096  HOUSEHOLD_1   \n",
       "2016-05-01      FOODS_3_154_WI_2_evaluation      FOODS_3_154      FOODS_3   \n",
       "\n",
       "               cat_id store_id state_id     d  sales  wm_yr_wk  wday  ...  \\\n",
       "date                                                                  ...   \n",
       "2011-03-13      FOODS     WI_2       WI    44      4     11107     2  ...   \n",
       "2011-03-13      FOODS     WI_2       WI    44     11     11107     2  ...   \n",
       "2011-03-13      FOODS     CA_3       CA    44     10     11107     2  ...   \n",
       "2011-03-13      FOODS     CA_3       CA    44     19     11107     2  ...   \n",
       "2011-03-13      FOODS     CA_3       CA    44     29     11107     2  ...   \n",
       "...               ...      ...      ...   ...    ...       ...   ...  ...   \n",
       "2016-05-01    HOBBIES     CA_1       CA  1920      2     11614     2  ...   \n",
       "2016-05-01      FOODS     WI_2       WI  1920     13     11614     2  ...   \n",
       "2016-05-01    HOBBIES     CA_1       CA  1920      8     11614     2  ...   \n",
       "2016-05-01  HOUSEHOLD     CA_3       CA  1920     23     11614     2  ...   \n",
       "2016-05-01      FOODS     WI_2       WI  1920     15     11614     2  ...   \n",
       "\n",
       "            pct_rmean7_lag14  pct_rstd7_lag14  pct_rmean14_lag14  \\\n",
       "date                                                               \n",
       "2011-03-13         -0.000000         0.479248           0.040680   \n",
       "2011-03-13         -0.000000         1.176758          -0.294922   \n",
       "2011-03-13         -0.081848         0.466553          -0.040924   \n",
       "2011-03-13          0.039124         0.437744          -0.007820   \n",
       "2011-03-13         -0.044952         0.427979           0.028091   \n",
       "...                      ...              ...                ...   \n",
       "2016-05-01         -0.101807         1.247070           0.025452   \n",
       "2016-05-01          0.038239         0.389893           0.019119   \n",
       "2016-05-01         -0.152466         0.814453          -0.060974   \n",
       "2016-05-01          0.040649         0.775879           0.045746   \n",
       "2016-05-01          0.048920         0.984863           0.030579   \n",
       "\n",
       "            pct_rstd14_lag14  pct_rmean28_lag14  pct_rstd28_lag14  \\\n",
       "date                                                                \n",
       "2011-03-13          0.491211           0.015251          0.382324   \n",
       "2011-03-13          2.830078           0.016388          3.205078   \n",
       "2011-03-13          0.540039          -0.025574          0.492920   \n",
       "2011-03-13          0.530762           0.017609          0.471924   \n",
       "2011-03-13          0.378662          -0.002810          0.410889   \n",
       "...                      ...                ...               ...   \n",
       "2016-05-01          1.122070          -0.012726          1.038086   \n",
       "2016-05-01          0.390625           0.016724          0.449219   \n",
       "2016-05-01          0.787109           0.007622          1.285156   \n",
       "2016-05-01          0.669434           0.012703          0.601074   \n",
       "2016-05-01          0.902832           0.052002          0.949219   \n",
       "\n",
       "            pct_price_mean  pct_price_std  pct_price_max  pct_price_min  \n",
       "date                                                                     \n",
       "2011-03-13        0.000000        0.00000       0.000000            0.0  \n",
       "2011-03-13        0.000000        0.00000       0.000000            0.0  \n",
       "2011-03-13        0.000000        0.00000       0.000000            0.0  \n",
       "2011-03-13        0.000000        0.00000       0.000000            0.0  \n",
       "2011-03-13        0.001553        0.00581       0.021744            0.0  \n",
       "...                    ...            ...            ...            ...  \n",
       "2016-05-01        0.000000        0.00000       0.000000            0.0  \n",
       "2016-05-01        0.000000        0.00000       0.000000            0.0  \n",
       "2016-05-01        0.000000        0.00000       0.000000            0.0  \n",
       "2016-05-01        0.000000        0.00000       0.000000            0.0  \n",
       "2016-05-01        0.000000        0.00000       0.000000            0.0  \n",
       "\n",
       "[819304 rows x 49 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce_mem_usage(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 819304 entries, 2011-03-13 to 2016-05-01\n",
      "Data columns (total 49 columns):\n",
      " #   Column             Non-Null Count   Dtype   \n",
      "---  ------             --------------   -----   \n",
      " 0   id                 819304 non-null  category\n",
      " 1   item_id            819304 non-null  category\n",
      " 2   dept_id            819304 non-null  category\n",
      " 3   cat_id             819304 non-null  category\n",
      " 4   store_id           819304 non-null  category\n",
      " 5   state_id           819304 non-null  category\n",
      " 6   d                  819304 non-null  int16   \n",
      " 7   sales              819304 non-null  int16   \n",
      " 8   wm_yr_wk           819304 non-null  int16   \n",
      " 9   wday               819304 non-null  int8    \n",
      " 10  month              819304 non-null  int8    \n",
      " 11  year               819304 non-null  int16   \n",
      " 12  snap_CA            819304 non-null  int8    \n",
      " 13  snap_TX            819304 non-null  int8    \n",
      " 14  snap_WI            819304 non-null  int8    \n",
      " 15  event_name_1       819304 non-null  int8    \n",
      " 16  event_type_1       819304 non-null  int8    \n",
      " 17  event_name_2       819304 non-null  int8    \n",
      " 18  event_type_2       819304 non-null  int8    \n",
      " 19  day                819304 non-null  int8    \n",
      " 20  week               819304 non-null  int8    \n",
      " 21  sell_price         819304 non-null  float16 \n",
      " 22  pct_shift_7        819304 non-null  float16 \n",
      " 23  pct_shift_8        819304 non-null  float16 \n",
      " 24  pct_shift_9        819304 non-null  float16 \n",
      " 25  pct_shift_10       819304 non-null  float16 \n",
      " 26  pct_shift_11       819304 non-null  float16 \n",
      " 27  pct_shift_12       819304 non-null  float16 \n",
      " 28  pct_shift_13       819304 non-null  float16 \n",
      " 29  pct_shift_14       819304 non-null  float16 \n",
      " 30  pct_shift_28       819304 non-null  float16 \n",
      " 31  pct_shift_35       819304 non-null  float16 \n",
      " 32  pct_shift_42       819304 non-null  float16 \n",
      " 33  pct_rmean7_lag7    819304 non-null  float16 \n",
      " 34  pct_rstd7_lag7     819304 non-null  float16 \n",
      " 35  pct_rmean14_lag7   819304 non-null  float16 \n",
      " 36  pct_rstd14_lag7    819304 non-null  float16 \n",
      " 37  pct_rmean28_lag7   819304 non-null  float16 \n",
      " 38  pct_rstd28_lag7    819304 non-null  float16 \n",
      " 39  pct_rmean7_lag14   819304 non-null  float16 \n",
      " 40  pct_rstd7_lag14    819304 non-null  float16 \n",
      " 41  pct_rmean14_lag14  819304 non-null  float16 \n",
      " 42  pct_rstd14_lag14   819304 non-null  float16 \n",
      " 43  pct_rmean28_lag14  819304 non-null  float16 \n",
      " 44  pct_rstd28_lag14   819304 non-null  float16 \n",
      " 45  pct_price_mean     819304 non-null  float16 \n",
      " 46  pct_price_std      819304 non-null  float16 \n",
      " 47  pct_price_max      819304 non-null  float16 \n",
      " 48  pct_price_min      819304 non-null  float16 \n",
      "dtypes: category(6), float16(28), int16(4), int8(11)\n",
      "memory usage: 72.7 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# split using only pandas\n",
    "\n",
    "remove_cols = ['sales', 'revenue'] \n",
    "cat_cols = ['id','item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
    "num_cols = [c for c in df.columns.tolist() if c not in (remove_cols+cat_cols)]\n",
    "used_col = cat_cols+num_cols\n",
    "    \n",
    "    \n",
    "y = df['sales']\n",
    "X = df[used_col]\n",
    "del df\n",
    "gc.collect()\n",
    "\n",
    "# Split train, test(last 7 days) # new\n",
    "y_train, y_test = y[:'2016-04-24'], y['2016-04-25':]\n",
    "del y\n",
    "gc.collect()\n",
    "\n",
    "X_train, X_test = X[:'2016-04-24'], X['2016-04-25':]\n",
    "del X\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "RMSE_train: 13.412612449045945\n",
      "RMSE_test: 9.466055189574256\n",
      "--------------------\n",
      "\n",
      "Naive benchmarks:\n",
      "Predicted Test week average sales: 11.593260550828024\n",
      "True Average Sales at:\n",
      "Test week: 12.5379764189745\n",
      "--------------------\n",
      "time: 3.138801662127177 min\n"
     ]
    }
   ],
   "source": [
    "#Predict using lgb model\n",
    "# y_pred = v_lin_reg(X_train, X_test, y_train, y_test)\n",
    "y_pred = predict_lgb(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(y_pred)\n",
    "result.to_pickle('wk_1_pred_1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict using SGD\n",
    "#y_pred = sgd(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict using lgb\n",
    "#y_pred = predict_lgb(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
